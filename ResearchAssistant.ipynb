{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39a8769-abcf-4ace-bd0b-62d9e0bbcdbd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54207c9d-e728-4d13-bd2d-d09e1f376de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fd5302-7cfd-4c29-b175-27c21c7aa1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key\n",
    "api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize models. We'll use a smaller LLM for most of the work, but a large LLM to distill conversations and writing the final report\n",
    "fast_llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "slow_llm = ChatAnthropic(api_key=api_key, model=\"claude-3-5-sonnet-20240620\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb56c0-c0e9-463f-b476-ec58c0a4b25a",
   "metadata": {},
   "source": [
    "# Generating the Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91439e-629b-4886-a3f4-f38746268ffd",
   "metadata": {},
   "source": [
    "## Generate Conversations and Resources\n",
    "\n",
    "In this section we will fetch related articles and develop a set of personas based on the outlines of the fetched related articles. Once the personas have been created, we will then pass them one by one into a conversation cycle.\n",
    "\n",
    "In the conversation cycle, the persona will ask questions about the topic to an expert on the topic. The expert will take their questions, split it into search queries, then find resources online based on these queries. The expert will then sift through these, collect the trusted sources, and answer the question using these sources. The persona will read the response and ask another question until they have no more questions. The conversation will be saved for the creation of the final outline.\n",
    "\n",
    "Once all of the personas have conversed with the expert, we will have compiled a set of conversations and trusted resources to be used in the final outline.\n",
    "\n",
    "This section will be the bulk of the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cbc22d-3bb7-497a-8fe9-1ce5256f5383",
   "metadata": {},
   "source": [
    "### Fetch Related Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bbe73e-1e95-41f5-8131-504e3409192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelatedTopics(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"List of related topics to help in generating personas.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735034cc-ddd4-42bd-a5e5-b646d1e5d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"I'm writing a Wikipedia page for the topic mentioned below. \n",
    "Please identify and recommend some related subjects that might be interesting. \n",
    "I'm looking for related subjects that provide insights into interesting aspects commonly associated with this topic.\n",
    "\n",
    "Feel free to list things that are only tangentially related\n",
    "\n",
    "Please list as many subjects as you can.\n",
    "\n",
    "Topic of interest: {topic}\n",
    "\n",
    "make sure to call the RelatedTopics function.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "related_topics_chain = related_topics_prompt | fast_llm.with_structured_output(\n",
    "    RelatedTopics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f96c609-01ca-4566-8786-e1d2d686cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedTopics(topics=['Artificial Intelligence', 'Machine Learning', 'Automation', 'Robotics', 'Ethics in AI', 'Job Displacement', 'Universal Basic Income', 'Data Privacy', 'Algorithm Bias', 'Smart Cities', 'Climate Change', 'Sustainable Development', 'Future of Work', 'Digital Transformation', 'Human-Machine Interaction', 'AI Ethics', 'Social Implications of AI', 'Impact of AI on Healthcare', 'Environmental Sustainability', 'AI in Finance'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_topic = \"AI agents and the potential economic, social, and environmental impacts.\"\n",
    "related_topics = await related_topics_chain.ainvoke({\"topic\": example_topic})\n",
    "related_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f007691-af14-453d-b6b1-aecf64bb12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get related articles with wikipedia retriever\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8399c3-f8f2-43e6-b888-834467e7e58b",
   "metadata": {},
   "source": [
    "### Create Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5d7d0-294a-4234-8d1e-c3fea187b6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c68247-f7d4-431d-a817-1295163229e4",
   "metadata": {},
   "source": [
    "### Converse With Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06987812-8a7b-4f28-93b0-e35475221d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d55aa152-5d17-4fd4-9f2f-1c45884950c3",
   "metadata": {},
   "source": [
    "## Generate Draft Outline\n",
    "\n",
    "The draft outline is a rough draft of the outline based only on the task. The collected conversations and resources from above will then be used to refine the draft outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb8047-5863-469a-8081-6342c2454832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a7a4b1-63ea-4f8e-9a8a-4daceca92856",
   "metadata": {},
   "source": [
    "## Generate Final Outline\n",
    "\n",
    "Use the collected conversations and resources to refine the draft outline. The result will be the final outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfde507-e80e-4194-b403-a96d7f0e8624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acdf6513-0305-484b-8f05-5b9c6d057bc0",
   "metadata": {},
   "source": [
    "# Generate the Final Article\n",
    "\n",
    "Each section will be generated independently using the previously gathered resources. Once each section is generated, we will have a large model go over the entire article to ensure consistency and remove duplicate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180d878-88c6-48ce-b0f6-ca1880b616be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
